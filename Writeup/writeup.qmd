---
title: "LLM Storage Writeup"
author: "Zachary Nicholas Houghton"
format: pdf
editor: visual
bibliography: references.bib
---

```{r setup, include=F}
library(tidyverse)
library(brms)
library(ggpubr)


gpt2_m4 = brm(log_odds_cosine ~ CenteredLogOverallFreq * RelFreq,
         data = cosine_data_m4,
         family = gaussian(),
         warmup = 2000,
         iter = 4000,
         cores = 4,
         chains = 4,
         #init = 0.1,
         file = '../Data/model4_gpt2')


gpt2_plot = conditional_effects(gpt2_m4, plot = F, effects = "CenteredLogOverallFreq:RelFreq", int_conditions=list(RelFreq = c(-0.25, 0, 0.25)))

gpt2xl_m4 = brm(log_odds_cosine ~ CenteredLogOverallFreq * RelFreq,
         data = cosine_data_m4,
         family = gaussian(),
         warmup = 2000,
         iter = 4000,
         cores = 4,
         chains = 4,
         #init = 0.1,
         file = '../Data/model4_gpt')

gpt2xl_plot = conditional_effects(gpt2xl_m4, plot = F, effects = "CenteredLogOverallFreq:RelFreq", int_conditions=list(RelFreq = c(-0.25, 0, 0.25)))

olmo1b_m4 = brm(log_odds_cosine ~ CenteredLogOverallFreq * RelFreq,
         data = cosine_data_m4,
         family = gaussian(),
         warmup = 2000,
         iter = 4000,
         cores = 4,
         chains = 4,
         #init = 0.1,
         file = '../Data/model4_olmo1b')

olmo1b_plot = conditional_effects(olmo1b_m4, plot = F, effects = "CenteredLogOverallFreq:RelFreq", int_conditions=list(RelFreq = c(-0.25, 0, 0.25)))

olmo7b_m4 = brm(log_odds_cosine ~ CenteredLogOverallFreq * RelFreq,
         data = cosine_data_m4,
         family = gaussian(),
         warmup = 2000,
         iter = 4000,
         cores = 4,
         chains = 4,
         #init = 0.1,
         file = '../Data/model4.1')


olmo7b_plot = conditional_effects(olmo7b_m4, plot = F, effects = "CenteredLogOverallFreq:RelFreq", int_conditions=list(RelFreq = c(-0.25, 0, 0.25)))


llama2_7b_m4 = brm(log_odds_cosine ~ CenteredLogOverallFreq * RelFreq,
         data = cosine_data_m4,
         family = gaussian(),
         warmup = 2000,
         iter = 4000,
         cores = 4,
         chains = 4,
         #init = 0.1,
         file = '../Data/model4_llama')


llama2_plot = conditional_effects(llama2_7b_m4, plot = F, effects = "CenteredLogOverallFreq:RelFreq", int_conditions=list(RelFreq = c(-0.25, 0, 0.25)))
```

<!--# What if we frame this as: Large Language Models make novel predictions in humans. Maybe we can write a larger opinion piece on the role of LLMs in linguistics? Can talk about the history of computational models, such as the development of the Rescorla-Wagner learning theory model, etc -->

# Introduction

In the last few years large language models have surged in popularity and have remained in the center of both the media and the recent research. With their surge in popularity has come many debates about to what extent they constitute as effective models of human language [e.g., @benderDangersStochasticParrots2021; @piantadosiMeaningReferenceLarge2022; @piantadosiChapterModernLanguage]. These questions have stemmed from clear differences in terms of both the training that the models receive as well as the performance of these models on language processing tasks. For example, common criticisms include their insanely large training size (sometimes being trained on upwards of 15 billion tokens), the potentially unrealistic nature of their tokenization (e.g., Chat GPT tokenizes kite as *k*, *ite*[^1]), and their poor performance on tasks that are trivial to humans (e.g., counting the number of *r*'s in *strawberry*[^2]).

[^1]: <https://tiktokenizer.vercel.app/>

[^2]: <https://community.openai.com/t/incorrect-count-of-r-characters-in-the-word-strawberry/829618>

Many of these debates are centered around the extend to which large language models are actually learning something abstract from the data and to what extent they are simply regurgitating their training data. Research has demonstrated mixed results with respect to the extent that they are copying from their input. For example, @haley2020bert demonstrated that many BERT models are not able to reliably determine the correct plurality of novel words. Similarly, @liAreNeuralNetworks2021 demonstrated that BERT tends to rely on memorization from its training data when producing the correct tense of novel words.

In contrast, recent research has demonstrated BERT's ability to generalize well to novel subject-verb pairs [@lasri2022subject; @liAssessingCapacityTransformer2023] and to use abstract knowledge to predict object-past participle agreements in French [@liAssessingCapacityTransformer2023]. Similarly, @mccoy2023much demonstrated that while GPT-2 copies extensively, it also produces both novel words as well as novel syntactic structures.

Given the evidence that large language models show evidence of both learning abstract knowledge as well as copying extensively from their training data, it's unclear in what situations they are leveraging their stored knowledge versus leveraging their more abstract knowledge. Thus the present study examines whether large language models are simply memorizing items or learning similar representations for words that contain the same tokens. Specifically, we use binomials (N *and* N compounds) as a test case since binomials can express the same (or very similar) meaning regardless of the ordering of the nouns.

## Binomials as a Test Case

Binomials are conjoined nouns (e.g., *bread and butter*) and provide a good test case because while the ordering of the nouns doesn't affect the meaning much (*cats and dogs* vs *dogs and cats*), humans have varying preferences about the ordering of the binomials [@benorChickenEggProbabilistic2006; @siyanova-chanturiaSeeingPhraseTime2011; @morganAbstractKnowledgeDirect2016; @morganModelingIdiosyncraticPreferences2015; @morganFrequencydependentRegularizationIterated2016a; @morganProductiveKnowledgeItemspecific2024]. For example, @benorChickenEggProbabilistic2006 demonstrated that a variety of different factors affect binomial ordering preferences, including phonological constraints such as stress and semantic constraints such as which term is more culturally significant. Additionally, @morganAbstractKnowledgeDirect2016 demonstrated that human ordering preferences for binomials are affected both by the relative frequency of the binomial (i.e., which ordering is more frequent) as well as abstract ordering preferences such as a preference for short words before long words.

By leveraging the fact that binomials can have two different orderings with the same meaning, we can examine whether large language models learn separate representations for them. Further, we can examine how their representations change as a function of overall frequency of the binomial (the frequency of the binomial regardless of order) and relative frequency. Specifically, a large language model may conceivably learn separate representations for binomials with a high overall-frequency (since it has had a lot of experience with the binomial) but may not learn separate representations for low-frequency binomials (since it has not had much experience with the binomial).

## Present Study

Since humans rely more on abstract knowledge for lower frequency items and rely more on their experience with the binomial for high-frequency binomials [@morganProductiveKnowledgeItemspecific2024], a natural consequence of this is that they have learned separate representations for high-frequency binomials. If large language models are doing something similar, than they may also learn separate representations for high-frequency binomials but not for lower-frequency binomials.

The present study addresses this question by examining the semantic representations of binomials varying in relative frequency and overall frequency. We examine the embeddings for both ordering of binomials in a sentence context, as well as examine the embeddings for a compositional form of the binomial (which we will elaborate on in the methods section). We hypothesize that the representations of the more frequent form (higher relative frequency form) for binomials with a high overall frequency may diverge more from the compositional representation than the less frequent ordering (lower relative frequency form) does for the same binomial. That is, for high-frequency binomials, the representation for the more frequent ordering may be more different from the compositional representation than the less-frequent ordering. For lower-frequency binomials, large language models may not learn different representations for the different orderings of the same form, regardless of the relative frequency.

In Experiment 1 we examine the representations of different binomials across different large language models and in Experiment 2 we examine the timecourse of these representations across each hidden layers for OLMo's 1B model [@groeneveldOLMoAcceleratingScience2024].

# Experiment 1

In Experiment 1 we examine the representations of binomials for GPT-2, GPT-2 XL [@radfordLanguageModelsAre2019], OLMo-1B, OLMo-7B [@groeneveldOLMoAcceleratingScience2024], and Llama2-7B [@touvronLlama2Open2023]. We examine the representations for different binomials in sentence contexts as well as the compositional representations of those same binomials. We explain these metrics in detail below.

## Methods

### Dataset

Our dataset consists of 784 sentences containing binomials. The sentences have been annotated for both relative frequency and overall frequency. Relative frequency is operationalized as the proportion of occurrences in alphabetical order (a neutral reference order) to occurrences in nonalphabetical order. Overall frequency is operationalized as the count of *A and B* plus the count of *B and A*. Counts were obtained using the Google *n-*grams corpus [@linSyntacticAnnotationsGoogle2012].

### Semantic Embeddings

In order to examine the semantic compositionality of binomials, we examined the semantic embeddings of five different large language models: GPT-2, GPT-2 XL [@radfordLanguageModelsAre2019], Llama-2 7B [@touvronLlama2Open2023], OLMo 1B and OLMo 7B [@groeneveldOLMoAcceleratingScience2024].[^3]

[^3]: All of our code can be found publicly available at \url{https://github.com/znhoughton/LLM-Storage}.

For each LLM we examined the semantic embeddings of the binomials in a sentence context. We accomplished this by passing the sentence through each large language model and extracting the second-to-last hidden layer for each of the words in the binomial. Since LLMs generate an embedding for each word, we computed the mean of these embeddings to represent the semantic embedding of the entire binomial in a sentence context (hereafter referred to as holistic embeddings). Next, we obtained the embedding for each word in the binomial individually, outside of a sentence context. We then computed the mean of these embeddings to represent the semantic embedding of the compositional form of the binomial (hereafter referred to as the compositional embeddings).

We then measured the cosine similarity between the holistic embeddings and the compositional embeddings for the alphabetical and nonalphabetical forms of each binomial. This is presented mathematically in @eq-cosalpha and @eq-cosnonalpha, where $cos_\alpha$ is the cosine similarity between the holistic embeddings of the alphabetical form of the binomial and the compositional form, $cos_{\neg\alpha}$ is the cosine similarity between the embeddings of the nonalphabetical form of the binomial and the compositional form, $h_\alpha$ and $h_{\neg\alpha}$ are the embeddings of the holistic form of the binomial in alphabetical and nonalphabetical forms respectively (in a sentence context), and $c$ is the embeddings of the compositional form. Since $c$ represents the mean of the embeddings for each word in the binomial out of context, order does not matter. Cosine similarity ranges from -1 to 1 where 1 indicates two extremely similar vectors and -1 indicates two extremely dissimilar vectors.

$$
\cos \alpha = \frac{\mathbf{h_\alpha} \cdot \mathbf{c}}{\|\mathbf{h_\alpha}\| \|\mathbf{c}\|}
$$ {#eq-cosalpha}

$$
\cos \neg\alpha = \frac{\mathbf{h_{\neg\alpha}} \cdot \mathbf{c}}{\|\mathbf{h_{\neg\alpha}}\| \|\mathbf{c}\|}
$$ {#eq-cosnonalpha}

For each binomial, we then calculated $LogCosSim$ which is the logged quotient of $cos_\alpha$ and $cos_{\neg\alpha}$ (@eq-cossim). A larger positive value indicates a greater degree of similarity between the holistic embeddings for the alphabetical form and the embeddings of the compositional form (i.e., the holistic embeddings of the alphabetical form are more similar to the embeddings of the compositional form than the holistic embeddings of the nonalphabetical form are) and a larger negative value represents the opposite.

$$
LogCosSim = log(\frac{cos_\alpha}{cos_{\neg\alpha}})
$$ {#eq-cossim}

### Analysis

We used a Bayesian mixed-effects model to examine how the semantic similarity between the holistic embeddings and the compositional embeddings trade off as a function of relative and overall frequency. Specifically, we modeled $LogCosSim$ as a function of overall frequency, which was centered and logged, $RelFreq$ which ranged from -0.5 to 0.5 (with 0.5 representing a binomial that appears only in the alphabetical form, and -0.5 representing a binomial that appears only in the nonalphabetical form), and their interaction. Our model is presented below in @eq-m4.

$$
LogCosSim \sim OverallFreq*RelFreq
$$ {#eq-m4}

## Results

The results of our models for each LLM are presented below. For all of our models, following [@houghtonTaskdependentConsequencesDisfluency2024] we report the percentage of posterior samples greater than zero. Since we are using Bayesian mixed-effects models, we are not forced into a binary of significant or non-significant. By reporting the percentage of posterior samples greater than zero, we can present a more nuanced picture of our results.

### GPT-2

Our mixed-effects model is presented below in @tbl-gpt2modelresults and visualized in @fig-gpt2. There was a meaningful main-effect of relative frequency ($\beta=-0.035$), suggesting that as relative frequency increases (i.e., for binomials with an increasing preference for the alphabetical form), the holistic embeddings for the alphabetical form are *less* similar to the compositional embeddings than the nonalphabetical holistic embeddings are. Further, there was a meaningful interaction effect between overall frequency and relative frequency ($\beta=-0.005$), suggesting that for high-frequency binomials there is a stronger effect of relative frequency than for low-frequency binomials. Specifically, for high-frequency binomials, those with a larger relative frequency have a lower $LogCosSim$ value. That is, for high-frequency binomials, the more preferred ordering's holistic embeddings are less similar to the compositional embeddings than the less preferred ordering's holistic embeddings are.

Our results suggest that GPT-2 is learning separate representations for high-frequency binomials, but may not be learning separate representations for low-frequency binomials.

```{r, echo = F, message = F, results='asis'}
#| label: tbl-gpt2modelresults
#| tbl-cap: "Model results for our Bayesian mixed-effects model for GPT-2."


percent_greater_zero = data.frame(fixef(gpt2_m4, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

summary_table4 = as.data.frame(fixef(gpt2_m4)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) 

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'CenteredLogOverallFreq', 'RelFreq', 'CenteredLogOverallFreq:RelFreq')))


summary_table4 = summary_table4 %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)

rownames(summary_table4) = c('Intercept', 'OverallFreq', 'RelFreq', 'OverallFreq:RelFreq')

knitr::kable(summary_table4, booktabs = T)


```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-gpt2
#| fig-cap: "Visualization of our model predictions for GPT-2 at relative frequency values of -0.25, 0, and 0.25."


gpt2 = gpt2_plot[[1]] %>%
  ggplot(aes(x=CenteredLogOverallFreq, y = estimate__, color = factor(RelFreq))) +
  geom_smooth(method='lm', formula=y~x, se=F) +
  geom_ribbon(aes(ymin=lower__, ymax = upper__, fill = factor(RelFreq)), alpha = 0.5) +
  ylab ('Log Cosine Similarity') +
  xlab('Overall Frequency') +
  #ggtitle('GPT2 Model') +
  theme_bw()

gpt2
```

### GPT-2 XL

Our mixed-effects model is presented below in @tbl-gpt2xlmodelresults and visualized in @fig-gpt2xl. We found a meaningful main-effect for overall frequency ($\beta=0.002$), though this seems to be driven largely by our interaction effect. We also found a meaningful interaction effect ($\beta=0.003$) between relative frequency and overall frequency, suggesting that for higher-frequency binomials, the holistic embeddings for the alphabetical form were *more* similar to the compositional form than the holistic embeddings for the nonalphabetical form were.

```{r, echo = F, message = F, results='asis'}
#| label: tbl-gpt2xlmodelresults
#| tbl-cap: "Model results for our Bayesian mixed-effects model for GPT-2 XL."


percent_greater_zero = data.frame(fixef(gpt2xl_m4, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

summary_table4 = as.data.frame(fixef(gpt2xl_m4)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) 

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'CenteredLogOverallFreq', 'RelFreq', 'CenteredLogOverallFreq:RelFreq')))


summary_table4 = summary_table4 %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)

rownames(summary_table4) = c('Intercept', 'OverallFreq', 'RelFreq', 'OverallFreq:RelFreq')

knitr::kable(summary_table4, booktabs = T)


```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-gpt2xl
#| fig-cap: "Visualization of our model predictions for GPT-2 XL at relative frequency values of -0.25, 0, and 0.25."


gpt2xl = gpt2xl_plot[[1]] %>%
  ggplot(aes(x = CenteredLogOverallFreq, y = estimate__)) +
  geom_smooth(aes(color = factor(RelFreq)), method = 'lm', formula = y ~ x, se = FALSE, show.legend = FALSE) +
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = factor(RelFreq)), alpha = 0.5) +
  ylab('Log Cosine Similarity') +
  xlab('Overall Frequency') +
  scale_fill_discrete(name = "RelFreq") +
  theme_bw()

gpt2xl
```

### OLMo-1B

Our mixed-effects model is presented below in @tbl-olmo1bmodelresults and visualized in @fig-olmo1b. We found a meaningful main-effect of relative frequency ($\beta=-0.152$), suggesting that for binomials with a stronger preference for the alphabetical form, the holistic embeddings of the alphabetical form were less similar to the compositional form than the holistic embeddings of the nonalphabetical form were. We also found a meaningful interaction effect ($\beta=-0.017$), suggesting that for lower-frequency binomials there is not much of a difference between the alphabetical and nonalphabetical forms in terms of their semantic embeddings, however for more-frequent binomials that occur more in the alphabetical form, the holistic embeddings for the alphabetical form are *less* similar to the compositional form than the holistic embeddings for the nonalphabetical form.

Our results suggest that Olmo-1B, similar to GPT2, is learning separate representations for high-frequency binomials, but may not be learning separate representations for low-frequency binomials.

```{r, echo = F, message = F, results='asis'}
#| label: tbl-olmo1bmodelresults
#| tbl-cap: "Model results for our Bayesian mixed-effects model for Olmo 1B."


percent_greater_zero = data.frame(fixef(olmo1b_m4, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

summary_table4 = as.data.frame(fixef(olmo1b_m4)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) 

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'CenteredLogOverallFreq', 'RelFreq', 'CenteredLogOverallFreq:RelFreq')))


summary_table4 = summary_table4 %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)

rownames(summary_table4) = c('Intercept', 'OverallFreq', 'RelFreq', 'OverallFreq:RelFreq')

knitr::kable(summary_table4, booktabs = T)


```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-olmo1b
#| fig-cap: "Visualization of our model predictions for Olmo 1B at relative frequency values of -0.25, 0, and 0.25."

olmo1b = olmo1b_plot[[1]] %>%
  ggplot(aes(x = CenteredLogOverallFreq, y = estimate__)) +
  geom_smooth(aes(color = factor(RelFreq)), method = 'lm', formula = y ~ x, se = FALSE, show.legend = FALSE) +
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = factor(RelFreq)), alpha = 0.5) +
  ylab('Log Cosine Similarity') +
  xlab('Overall Frequency') +
  scale_fill_discrete(name = "RelFreq") +
  theme_bw()

olmo1b
```

### OLMo-7B

Our mixed-effects model is presented below in @tbl-olmo7bmodelresults and visualized in @fig-olmo7b. We found a meaningful main-effect of relative frequency ($\beta=-0.151$), suggesting that for binomials with a stronger preference for the alphabetical form, the holistic embeddings of the alphabetical form were less similar to the compositional form than the holistic embeddings of the nonalphabetical form were. We also found a meaningful interaction effect ($\beta=-0.017$), suggesting that for lower-frequency binomials there is not much of a difference between the alphabetical and nonalphabetical forms in terms of their semantic embeddings, however for more-frequent binomials that occur more in the alphabetical form, the holistic embeddings for the alphabetical form are *less* similar to the compositional form than the holistic embeddings for the nonalphabetical form.

Our results suggest that Olmo-7B, similar to Olmo-1B and GPT2, is learning separate representations for high-frequency binomials, but may not be learning separate representations for low-frequency binomials.

```{r, echo = F, message = F, results='asis'}
#| label: tbl-olmo7bmodelresults
#| tbl-cap: "Model results for our Bayesian mixed-effects model for Olmo 7B."


percent_greater_zero = data.frame(fixef(olmo7b_m4, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

summary_table4 = as.data.frame(fixef(olmo7b_m4)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) 

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'CenteredLogOverallFreq', 'RelFreq', 'CenteredLogOverallFreq:RelFreq')))


summary_table4 = summary_table4 %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)

rownames(summary_table4) = c('Intercept', 'OverallFreq', 'RelFreq', 'OverallFreq:RelFreq')

knitr::kable(summary_table4, booktabs = T)


```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-olmo7b
#| fig-cap: "Visualization of our model predictions for Olmo 7B at relative frequency values of -0.25, 0, and 0.25."


olmo7b = olmo7b_plot[[1]] %>%
  ggplot(aes(x = CenteredLogOverallFreq, y = estimate__)) +
  geom_smooth(aes(color = factor(RelFreq)), method = 'lm', formula = y ~ x, se = FALSE, show.legend = FALSE) +
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = factor(RelFreq)), alpha = 0.5) +
  ylab('Log Cosine Similarity') +
  xlab('Overall Frequency') +
  scale_fill_discrete(name = "RelFreq") +
  theme_bw()

olmo7b
```

### Llama2-7B

Our mixed-effects model is presented below in @tbl-llama2modelresults and visualized in @fig-llama2. While the credible interval for the interaction effect crosses zero, over 96% of the posterior samples were less than zero, suggesting that there is a meaningful interaction effect. The results suggest that for lower-frequency binomials there is not much of a difference between the alphabetical and nonalphabetical forms in terms of their semantic embeddings, however for more-frequent binomials that occur more in the alphabetical form, the holistic embeddings for the alphabetical form are *less* similar to the compositional form than the holistic embeddings for the nonalphabetical form.

```{r, echo = F, message = F, results='asis'}
#| label: tbl-llama2modelresults
#| tbl-cap: "Model results for our Bayesian mixed-effects model for Llama2 7B."


percent_greater_zero = data.frame(fixef(llama2_7b_m4, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)
  

summary_table4 = as.data.frame(fixef(llama2_7b_m4)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) 

percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'CenteredLogOverallFreq', 'RelFreq', 'CenteredLogOverallFreq:RelFreq')))


summary_table4 = summary_table4 %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)

rownames(summary_table4) = c('Intercept', 'OverallFreq', 'RelFreq', 'OverallFreq:RelFreq')

knitr::kable(summary_table4, booktabs = T)


```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-llama2
#| fig-cap: "Visualization of our model predictions for Llama2 7B at relative frequency values of -0.25, 0, and 0.25."



llama2 = llama2_plot[[1]] %>%
  ggplot(aes(x = CenteredLogOverallFreq, y = estimate__)) +
  geom_smooth(aes(color = factor(RelFreq)), method = 'lm', formula = y ~ x, se = FALSE, show.legend = FALSE) +
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = factor(RelFreq)), alpha = 0.5) +
  ylab('Log Cosine Similarity') +
  xlab('Overall Frequency') +
  scale_fill_discrete(name = "RelFreq") +
  theme_bw()

llama2
```

## Discussion

Overall our results suggest that for higher frequency binomials in most large language models, the semantic representation for the more frequent form of the binomial diverges more from the representation of the compositional form. This suggests that large language models tend to learn different representations for high-frequency binomials, similar to what has been argued that humans do [@morganAbstractKnowledgeDirect2016]. However, it's unclear on what timescale this emerges and at what hidden layers this result holds for. For example, does this difference emerge early in training or does it take a large amount of training for these different representations to emerge? Further, since different layers have been proposed to correspond to different functions [e.g., earlier layers may represent more phonological knowledge while later layers may represent more semantic knowledge; @tenney2019bert], it is possible that these results may be vary across different layers. In Experiment 2 we examine both of these questions.

# Experiment 2

Experiment 2 is an exploratory analysis examining how representations for binomials emerge throughout training across different hidden layers. Specifically, since OLMo [@groeneveldOLMoAcceleratingScience2024] released the model's checkpoints at various stages in the training we can examine how our results in Experiment 1 emerge throughout training. Further, since the model is open access we can also examine the different hidden-layers of the model.

## Methods

The methods in Experiment 2 were almost identical to those used in Experiment 1, with two main exceptions: first, rather than examining several different large language models, we instead examined a single large language model: OlmO 1B. OlmO 1B has released checkpoints at different stages in learning. As such, we can examine the representations of binomials at different stages of learning. Second, we also examined the representations at each hidden layer in the model in order to examine how the representation changes across layers.

For the present study, we examine the embeddings for our sentences from Experiment 1 at each hidden layer at multiple different steps in the training. In addition to examining the model after being trained, we also examine the embeddings after being trained for 20000 (84B tokens), 40000 (168B tokens), 60000 (252B tokens), 80000 (336B tokens), and 100000 (419B tokens) steps.

## Results

A visualization of the embeddings at different layers and different checkpoints is included below:

```{r echo = F, fig.width = 16, fig.height = 12, fig.align = 'center', fig.fullwidth = TRUE, warning = FALSE, message = FALSE}

#| column: screen
#| out-width: 100%
#| fig-format: svg

aggregate_data = read_csv('../Data/aggregate_data.csv')
aggregate_data = aggregate_data %>%
  filter(checkpoint %in% c('20000', '40000', '60000', '80000', '100000', 'main'))

aggregate_data$checkpoint = factor(aggregate_data$checkpoint, levels = c('20000', '40000', '60000', '80000', '100000', 'main'))


aggregate_data = aggregate_data %>%
  mutate(RelFreq = ifelse(RelFreq < 0, "nonalpha", "alpha"))

aggregate_data_plot = ggplot(aggregate_data, aes(
     x = CenteredLogOverallFreq,
     y = log_odds_cosine,
     color = RelFreq
   )) +
     geom_point(alpha = 0.2) +
     geom_smooth(method = 'lm', formula = y ~ x, se = TRUE, linewidth = 1) +
     ylab('Log Odds Cosine') +
     xlab('Centered Log Overall Frequency') +
     facet_grid(checkpoint ~ layer) +
     theme_bw() +
     scale_color_manual(
       values = c("nonalpha" = "turquoise3", "alpha" = "deeppink2"),  # Ensure matching labels
       name = "RelFreq"
     ) +
     scale_x_continuous(breaks = c(-5, 5)) +  # Only show -5 and 5 as x-axis labels
     coord_cartesian(ylim = c(-0.5, 0.5)) +
     theme(
       axis.title = element_text(size = 20),
       axis.text = element_text(size = 16),
       legend.title = element_text(size = 18),
       legend.text = element_text(size = 16),
       strip.text = element_text(size = 18)
     )

aggregate_data_plot


```

There are two trends that are notable. First, the earlier layers tend to display an opposite pattern from the later layers, with the more frequent embeddings being more similar to the compositional form. Second, the holistic representation of the frequent ordering diverges from the compositional representation at about the 80000th step (336B tokens). We will discuss the implications of both of these in the discussion section.

## Discussion

Our results demonstrate that from early on in the training the frequency difference is reflected in the embeddings in the early layers. Interestingly, however, this is not reflected in the representation at later layers. Instead, the differences in representations emerge in later layers over time.

These results are consistent with the general idea that later layers encode more semantic information, since the pattern of the more frequent embeddings diverging is seen in the later layers, while the earlier layers show the opposite pattern.

Additionally, the fact that the pattern emerges rather slowly (taking several hundred billion tokens of training) suggests that the model must experience the binomial quite a lot in order to learn a separate representation for it. This suggests that the model isn't simply learning a separate representation because the binomial occurs in different semantic contexts (because if that were the case we would see the pattern emerge quite early on), but because it occurs frequently. This is in line with usage-based theories that have argued that holistic representations in humans emerge as a function of usage [e.g., @bybeePhonologyLanguageUse2003].

# Conclusion

The present study demonstrates that the semantic embeddings for the more frequent ordering of a given binomial become less similar to the compositional embeddings as a function of the overall frequency of the binomial. That is, the embeddings of the more frequent ordering of a high-frequency binomial (e.g., *bread and butter*) are less similar to the compositional embeddings than the less frequent ordering's embedidngs (e.g., *butter and bread*) are. Another way to frame these results is that the same form (i.e., the same words) can give rise to quantitatively (but systematically) different representations in large language models and this varies depending on the overall frequency of that form (in either order).

It may not seem particularly surprising that the more frequent form diverges in semantic representation from the compositional form. After all, by definition a large language model has more experience with the more frequent form, which means the embeddings are being updated more often for the more frequent ordering. This in turn creates more opportunities for those embeddings to diverge from the compositional embeddings. However, what is interesting is how this effect emerges over time: early on in the training, the embeddings for the more frequent form are more similar to the compositional form across both earlier and later layers. Further, as training continues this stays the case for early layers, but undergoes a reversal in later layers.

One possible explanation for our results is that the more frequent form may be occurring in particularly different contexts from the compositional and less frequent forms (e.g., perhaps they are more idiomatic, such as *black and white*[^4]). However, if this were the case then we would expect to see the embeddings for the frequent form to diverge from the embeddings of the compositional form quite early in training. Instead, however, we actually see the opposite early in the training: the embeddings for the more frequent form are *more* similar to those of the compositional form and it takes time for these embeddings to diverge.

[^4]: Although all of our sentences were sentences that encouraged a compositional reading of our binomials, and very few of our binomials had a particularly idiomatic meaning to begin with.

Another possibility is that early in training for high-frequency binomials, the large language model's experience with the individual words may largely overlap with the large language model's experience with the frequent form of the binomial (e.g., the model's experience with contexts containing the binomial *bread and butter* are also contributing to the large language model's experience with the individual words). Thus, initially these embeddings may be similar until the large language model experiences enough data to learn different representations. As the model experiences more sentence contexts with the binomial, the representation for the more frequent ordering has more opportunities to diverge from the representation of the individual words. This process explains why the same form can give rise to different representations.

Finally, our results can also be considered predictions for how humans may learn representations. Future work would do well to examine whether it is also the case that the semantic representations for the more frequent ordering of high-frequency binomials diverge more from the compositional representations in humans. Our results also make predictions about the timescale of learning: for young children, the pattern of results may actually be the opposite from adults, since at earlier checkpoints in our model the embeddings for the more frequent ordering of high-frequency binomials were more similar to the compositional embeddings.

\clearpage
